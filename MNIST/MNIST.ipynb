{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.setdefaulttensortype('torch.FloatTensor')\n",
    "torch.manualSeed(666)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mnist = require 'mnist'\n",
    "\n",
    "train = mnist.traindataset()\n",
    "test = mnist.testdataset()\n",
    "\n",
    "local mean, std\n",
    "\n",
    "for _,set in ipairs{train, test} do\n",
    "    set.data = set.data:float()\n",
    "    \n",
    "    if not mean then\n",
    "        mean = train.data:mean()\n",
    "        std = train.data:std()\n",
    "    end\n",
    "    set.data:add(-mean)\n",
    "    set.data:div(std)\n",
    "    \n",
    "    set.label:add(1)\n",
    "end\n",
    "\n",
    "collectgarbage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-- ****** Convolutional net ******\n",
    "\n",
    "require 'nn'\n",
    "\n",
    "net = nn.Sequential()\n",
    "\n",
    "net:add(nn.SpatialConvolution(1, 64, 5, 5, 1, 1, 0, 0))\n",
    "net:add(nn.SpatialBatchNormalization(64))\n",
    "net:add(nn.ReLU())\n",
    "net:add(nn.SpatialConvolution(64, 64, 3, 3, 1, 1, 0, 0))\n",
    "net:add(nn.SpatialBatchNormalization(64))\n",
    "net:add(nn.ReLU())\n",
    "net:add(nn.SpatialConvolution(64, 64, 3, 3, 1, 1, 0, 0))\n",
    "net:add(nn.SpatialBatchNormalization(64))\n",
    "net:add(nn.ReLU())\n",
    "\n",
    "net:add(nn.SpatialMaxPooling(2, 2, 2, 2))\n",
    "\n",
    "net:add(nn.SpatialConvolution(64, 64, 3, 3, 1, 1, 0, 0))\n",
    "net:add(nn.SpatialBatchNormalization(64))\n",
    "net:add(nn.ReLU())\n",
    "net:add(nn.SpatialConvolution(64, 64, 3, 3, 1, 1, 0, 0))\n",
    "net:add(nn.SpatialBatchNormalization(64))\n",
    "net:add(nn.ReLU())\n",
    "net:add(nn.SpatialConvolution(64, 64, 3, 3, 1, 1, 0, 0))\n",
    "net:add(nn.SpatialBatchNormalization(64))\n",
    "\n",
    "net:add(nn.View(64, -1):setNumInputDims(3))\n",
    "net:add(nn.Mean(3, 3))\n",
    "net:add(nn.BatchNormalization(64))\n",
    "\n",
    "net:add(nn.Linear(64, 10))\n",
    "\n",
    "params, gradParams = net:getParameters()\n",
    "\n",
    "collectgarbage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-- ****** Linear classifier ******\n",
    "\n",
    "require 'nn'\n",
    "\n",
    "net = nn.Sequential()\n",
    "\n",
    "net:add(nn.View(-1):setNumInputDims(3))\n",
    "net:add(nn.Linear(28*28, 10))\n",
    "\n",
    "params, gradParams = net:getParameters()\n",
    "\n",
    "collectgarbage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-- ****** Haar features ******\n",
    "\n",
    "require 'nn'\n",
    "\n",
    "IntegralSmartNorm = nil\n",
    "debug.getregistry()['IntegralSmartNorm'] = nil \n",
    "package.loaded['IntegralSmartNorm'] = nil\n",
    "require 'IntegralSmartNorm'\n",
    "\n",
    "net = nn.Sequential()\n",
    "\n",
    "net:add(IntegralSmartNorm(1, 64, 28, 28))\n",
    "net:add(nn.SpatialConvolution(64, 64, 1,1, 4,4))\n",
    "net:add(nn.View(64*7*7):setNumInputDims(3))\n",
    "-- net:add(nn.Mean(3, 3))\n",
    "net:add(nn.Linear(64*7*7, 10))\n",
    "\n",
    "int = net:get(1)\n",
    "int.normalize = false\n",
    "int.exact = false\n",
    "int.updateGradInput = function() end --function(self, input) self.gradInput:resizeAs(input) end\n",
    "\n",
    "accBatch = true\n",
    "\n",
    "params, gradParams = net:getParameters()\n",
    "\n",
    "collectgarbage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "require 'optim'\n",
    "\n",
    "datasetIdx = 1\n",
    "\n",
    "optimState = {\n",
    "    learningRate = 1e-2,\n",
    "    momentum = 0.9,\n",
    "    nesterov = true,\n",
    "    dampening = 0\n",
    "}\n",
    "\n",
    "crit = nn.CrossEntropyCriterion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logger = optim.Logger('MNIST/mnist-acc-integral.log')\n",
    "logger:setNames{'Training loss'}\n",
    "logger:style{'-'}\n",
    "-- logger:showPlot(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function fillBatch(batch, labels, set, currIdx, idxs)\n",
    "    for k = 1,batch:size(1) do\n",
    "        batch[k]:copy(set.data[idxs and idxs[currIdx] or currIdx])\n",
    "        labels[k] = set.label[idxs and idxs[currIdx] or currIdx]\n",
    "        \n",
    "        currIdx = currIdx + 1\n",
    "        if currIdx > set.size then currIdx = 1 end\n",
    "    end\n",
    "    \n",
    "    return currIdx\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.3244831562042 sec\t\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "3.6126310825348 sec\t\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "4.262176990509 sec\t\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "4.1294159889221 sec\t\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2.984934091568 sec\t\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batchSize = 128\n",
    "batch = torch.FloatTensor(batchSize, 1, 28, 28)\n",
    "labels = torch.IntTensor(batchSize)\n",
    "\n",
    "function randomShuffle(t)\n",
    "  for i = 1,#t do\n",
    "    local j = math.random(i, #t)\n",
    "    t[i], t[j] = t[j], t[i]\n",
    "  end\n",
    "end\n",
    "\n",
    "idx = {}\n",
    "for i = 1,train.data:size(1) do\n",
    "    idx[i] = i\n",
    "end\n",
    "\n",
    "for iter = 1,5 do\n",
    "    local oldIdx = datasetIdx\n",
    "    datasetIdx = fillBatch(batch, labels, train, datasetIdx, idx)\n",
    "    if datasetIdx < oldIdx then\n",
    "        print('New epoch')\n",
    "        randomShuffle(idx)\n",
    "        datasetIdx = 1\n",
    "    end\n",
    "    \n",
    "    local loss\n",
    "    \n",
    "    timer = torch.Timer()\n",
    "    if accBatch then\n",
    "        net:zeroGradParameters()\n",
    "        loss = 0\n",
    "        for k = 1,batchSize do\n",
    "            loss = loss + crit:forward(net:forward(batch[k]), labels[k])\n",
    "            crit:backward(net.output, labels[k])\n",
    "            net:backward(batch[k], crit.gradInput, 1 / batchSize)\n",
    "        end\n",
    "        loss = loss / batchSize\n",
    "    else\n",
    "        loss = crit:forward(net:forward(batch), labels)\n",
    "        crit:backward(net.output, labels)\n",
    "        net:zeroGradParameters()\n",
    "        net:backward(batch, crit.gradInput)\n",
    "    end\n",
    "    print(timer:time().real .. ' sec')\n",
    "    \n",
    "    local feval = function(x)\n",
    "        return batchLoss, gradParams\n",
    "    end\n",
    "\n",
    "    optim.sgd(feval, params, optimState)\n",
    "    \n",
    "    logger:add{loss}\n",
    "    if iter % 1 == 0 or iter == 469 then\n",
    "        logger:plot()\n",
    "    end\n",
    "    \n",
    "    collectgarbage() \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " [..............................................]                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b ETA: 0ms | Step: 0ms\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b "
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.89568591117859 seconds\t\n",
       "6%\t\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-- Test\n",
    "require 'xlua'\n",
    "\n",
    "local testBatchSize = 100\n",
    "assert(test.size % testBatchSize == 0)\n",
    "\n",
    "local correctCount = 0\n",
    "local batchCount = 0\n",
    "local testDatasetIdx = 1\n",
    "local batch = torch.FloatTensor(testBatchSize, 1, 28, 28)\n",
    "local labels = torch.LongTensor(testBatchSize)\n",
    "\n",
    "-- timer = torch.Timer()\n",
    "repeat\n",
    "    batchCount = batchCount + 1\n",
    "    xlua.progress(testBatchSize * batchCount, test.size)\n",
    "    \n",
    "    testDatasetIdx = fillBatch(batch, labels, test, testDatasetIdx)\n",
    "    if accBatch then\n",
    "        for k = 1,testBatchSize do\n",
    "            net:forward(batch[k])\n",
    "            local answer = select(2, net.output:max(1))[1]\n",
    "            correctCount = correctCount + (answer == labels[k] and 1 or 0)\n",
    "        end\n",
    "    else\n",
    "        net:forward(batch)\n",
    "        local answer = select(2, net.output:max(2))\n",
    "        correctCount = correctCount + answer:eq(labels):sum()\n",
    "    end\n",
    "    \n",
    "until testDatasetIdx < testBatchSize\n",
    "-- print(timer:time().real .. ' seconds')\n",
    "\n",
    "print(correctCount / (batchCount * testBatchSize) * 100 .. '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One 100-batch forward pass time:\n",
    "* convnet: 1.357259035110\n",
    "* Haar features (normalize = true): 11.31303405761\n",
    "* normalize = false: 9.618772983551\n",
    "* no cv.integral: 9.422287940979\n",
    "* no forwardCFunction: 0.73264694213867\n",
    "* no OpenMP: 0.89568591117859"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net:clearState()\n",
    "torch.save('mnist-linear.t7', net)\n",
    "torch.save('mnist-linear-optimstate.t7', optimState)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iTorch",
   "language": "lua",
   "name": "itorch"
  },
  "language_info": {
   "name": "lua",
   "version": "5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
