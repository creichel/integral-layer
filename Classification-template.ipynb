{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.setdefaulttensortype('torch.FloatTensor')\n",
    "require 'cutorch'\n",
    "\n",
    "exact = true\n",
    "separateParams = false\n",
    "GPUs = {1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = require 'cifar10'\n",
    "\n",
    "dataset.relative = '../../Datasets/CIFAR-10/cifar10_whitened_float.t7'\n",
    "nClasses = dataset.nClasses -- 10\n",
    "\n",
    "trainFiles = dataset.loadNames('train')\n",
    "valFiles = dataset.loadNames('val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "require 'cunn'\n",
    "require 'cudnn'\n",
    "\n",
    "if #GPUs > 1 then\n",
    "    local function bnparams(self)\n",
    "       local p, gp = nn.Module.parameters(self)\n",
    "        p[#p+1] = self.running_mean\n",
    "        p[#p+1] = self.running_var\n",
    "        gp[#gp+1] = self.running_mean:clone():zero()\n",
    "        gp[#gp+1] = self.running_var:clone():zero()\n",
    "        return p, gp\n",
    "    end\n",
    "\n",
    "    torch.getconstructortable('nn.BatchNormalization').parameters = bnparams\n",
    "    torch.getconstructortable('cudnn.BatchNormalization').parameters = bnparams\n",
    "end\n",
    "\n",
    "cudnn.fastest = true\n",
    "cudnn.benchmark = true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelName = '000-wrn-40-2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Render images for displaying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3\t\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "Console does not support images"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 32,
       "width": 32
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1\t\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "Console does not support images"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 32,
       "width": 32
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "-- example blending\n",
    "local k = 19\n",
    "local img, label = dataset.loadSample(trainFiles[k])\n",
    "print(label)\n",
    "itorch.image(img)\n",
    "\n",
    "local k = 190\n",
    "local img, label = dataset.loadSample(trainFiles[k])\n",
    "print(label)\n",
    "itorch.image(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "intKind = 'IntegralSmartNorm'\n",
    "_G[intKind] = nil\n",
    "debug.getregistry()[intKind] = nil \n",
    "package.loaded[intKind] = nil\n",
    "\n",
    "require(intKind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Number of IntegralSmartNorm modules: 0\t\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local modelName = modelName\n",
    "net = assert(loadfile('ModelsClassification/' .. modelName .. '.lua'))(dataset.h, dataset.w, nClasses)\n",
    "\n",
    "ints = net:findModules(intKind)\n",
    "for i = 1,#ints do\n",
    "    ints[i].normalize = true\n",
    "    ints[i].exact = exact\n",
    "    ints[i].saveMemoryIntegral = false\n",
    "end\n",
    "print('Number of IntegralSmartNorm modules: ' .. #ints)\n",
    "\n",
    "net:cuda()\n",
    "collectgarbage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "math.randomseed(666)\n",
    "torch.manualSeed(666)\n",
    "if CUDA then cutorch.manualSeed(666) end\n",
    "\n",
    "if #GPUs > 1 then\n",
    "    net = nn.DataParallelTable(1, true, true):threads(\n",
    "        function()\n",
    "            require 'cudnn'\n",
    "            require 'IntegralSmartNorm'\n",
    "        end)\n",
    "        :cuda():add(net, GPUs)\n",
    "end\n",
    "\n",
    "net:reset()\n",
    "\n",
    "if separateParams then\n",
    "    params, gradParams = net:parameters()\n",
    "else\n",
    "    params, gradParams = net:getParameters()\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "require 'optim'\n",
    "\n",
    "datasetIdx = 1\n",
    "\n",
    "if separateParams then\n",
    "    optimStates = {}\n",
    "\n",
    "    local intParamsCount = 0\n",
    "\n",
    "    for i = 1,#params do\n",
    "        optimStates[i] = {\n",
    "            learningRate = 1e-3,\n",
    "            momentum = 0.9,\n",
    "            nesterov = true,\n",
    "            dampening = 0,\n",
    "            learningRateDecay = 0,\n",
    "            weightDecay = 2e-4\n",
    "        }\n",
    "\n",
    "        for k = 1,#ints do\n",
    "            if \n",
    "                params[i] == ints[k].xMin or\n",
    "                params[i] == ints[k].xMax or\n",
    "                params[i] == ints[k].yMin or\n",
    "                params[i] == ints[k].yMax then\n",
    "\n",
    "                ints[k].params = ints[k].params or {}\n",
    "                table.insert(ints[k].params, i)\n",
    "\n",
    "                optimStates[i].weightDecay = 2e-4 -- 0.0185\n",
    "                optimStates[i].learningRate = optimStates[i].learningRate\n",
    "                intParamsCount = intParamsCount + 1\n",
    "                break\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    print('Number of IntegralSmartNorm parameter tensors: ' .. intParamsCount)\n",
    "else\n",
    "    optimStates = {\n",
    "        learningRate = 1e-3,\n",
    "        momentum = 0.9,\n",
    "        nesterov = true,\n",
    "        dampening = 0,\n",
    "        learningRateDecay = 0,\n",
    "        weightDecay = 2e-4\n",
    "    }\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputDir = 'CIFAR-10 classification/' .. modelName .. '/001/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "-- do return end\n",
    "\n",
    "for k = 1,#ints do\n",
    "    for p = 1,ints[k].xMin:nElement() do\n",
    "        ints[k]:resetSingleWindow(p)\n",
    "    end\n",
    "    ints[k].xMin:mul(1.5)\n",
    "    ints[k].xMax:mul(1.5)\n",
    "    ints[k].yMin:mul(1.5)\n",
    "    ints[k].yMax:mul(1.5)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Load a saved model\n",
    "do return end\n",
    "require 'nngraph'\n",
    "require 'cunn'\n",
    "require 'cudnn'\n",
    "\n",
    "net, optimStates, GSconfig = table.unpack(torch.load(outputDir .. 'net.t7'))\n",
    "\n",
    "net:cuda()\n",
    "\n",
    "local poolings   = net:findModules('nn.SpatialMaxPooling')\n",
    "local unpoolings = net:findModules('nn.SpatialMaxUnpooling')\n",
    "for i = 1,#unpoolings do\n",
    "    unpoolings[i].pooling = poolings[#poolings-i+1]\n",
    "end\n",
    "\n",
    "ints = net:findModules(intKind)\n",
    "for i = 1,#ints do\n",
    "    ints[i].normalize = true\n",
    "    ints[i].exact = exact\n",
    "end\n",
    "\n",
    "require 'optim'\n",
    "\n",
    "loaded = true\n",
    "\n",
    "if #GPUs > 1 then\n",
    "    net = nn.DataParallelTable(1, true, true):threads(\n",
    "        function()\n",
    "            require 'cudnn'\n",
    "            require 'IntegralSmartNorm'\n",
    "        end)\n",
    "        :cuda():add(net, GPUs)\n",
    "end\n",
    "\n",
    "if separateParams then\n",
    "    params, gradParams = net:parameters()\n",
    "else\n",
    "    params, gradParams = net:getParameters()\n",
    "end\n",
    "\n",
    "datasetIdx = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "package.loaded.WindowDebugger = nil\n",
    "debug.getregistry().WindowDebugger = nil\n",
    "WindowDebugger = nil\n",
    "\n",
    "require 'WindowDebugger'\n",
    "\n",
    "-- ints = {1,1,1,1,1,1,1,1}\n",
    "wDebs = (#ints > 0) and {} or nil\n",
    "for k = 1,#ints do wDebs[k] = WindowDebugger() end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "require 'optim'\n",
    "\n",
    "onlineLossLogger = optim.Logger(outputDir .. 'onlineLossLog.log')\n",
    "evalLogger = optim.Logger(outputDir .. 'evalLog.log')\n",
    "evalAccLogger = optim.Logger(outputDir .. 'evalAccLog.log')\n",
    "\n",
    "onlineLossLogger:setNames{'Online batch loss'}\n",
    "evalLogger:setNames{'Train loss', 'Validation loss'}\n",
    "evalAccLogger:setNames{'Train error', 'Validation error'}\n",
    "\n",
    "onlineLossLogger:style{'-'}\n",
    "evalLogger:style{'-', '-'}\n",
    "evalAccLogger:style{'-', '-'}\n",
    "\n",
    "onlineLossLogger.showPlot = false\n",
    "evalLogger.showPlot = false\n",
    "evalAccLogger.showPlot = false\n",
    "\n",
    "function needToPlot(_onlineLossLogger)\n",
    "    local plotFreq = 100\n",
    "    local count = #_onlineLossLogger.symbols[1] + 1\n",
    "    while count > 1000 do\n",
    "        plotFreq = plotFreq * 10\n",
    "        count = count / 10\n",
    "    end\n",
    "    \n",
    "    return #_onlineLossLogger.symbols[1] % plotFreq == 0\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k = 1,#ints do\n",
    "    if wDebs and loaded and paths.filep(outputDir .. 'wd' .. k .. '.t7') then\n",
    "        wDebs[k]:load(outputDir .. 'wd' .. k .. '.t7')\n",
    "    end\n",
    "end\n",
    "\n",
    "if loaded and paths.filep(outputDir .. 'evalLogger.t7') then\n",
    "    evalLogger.symbols = torch.load(outputDir .. 'evalLogger.t7')\n",
    "end\n",
    "if loaded and paths.filep(outputDir .. 'evalAccLogger.t7') then\n",
    "    evalAccLogger.symbols = torch.load(outputDir .. 'evalAccLogger.t7')\n",
    "end\n",
    "if loaded and paths.filep(outputDir .. 'onlineLossLogger.t7') then\n",
    "    onlineLossLogger.symbols = torch.load(outputDir .. 'onlineLossLogger.t7')\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "do return end\n",
    "\n",
    "-- Measure forward/backward propagation time\n",
    "-- input = torch.CudaTensor(1, 3, dataset.h, dataset.w):fill(0.1)\n",
    "input = batch\n",
    "local exactTest = false\n",
    "net:evaluate()\n",
    "\n",
    "for k = 1,#ints do\n",
    "    ints[k].exact = exactTest\n",
    "end\n",
    "\n",
    "timer = torch.Timer()\n",
    "for k = 1,nRepeats do\n",
    "    net:forward(input)\n",
    "--     net:backward(input, net.output:clone())\n",
    "end\n",
    "cutorch.synchronize()\n",
    "\n",
    "for k = 1,#ints do\n",
    "    ints[k].exact = exact\n",
    "end\n",
    "\n",
    "print('Average time for 1 forward pass: ' .. (timer:time().real) .. ' seconds. Output size:')\n",
    "print(net.output:size())\n",
    "\n",
    "collectgarbage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyCriterion():cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluationIdxTrain = {}\n",
    "for k = 1,#trainFiles,5 do table.insert(evaluationIdxTrain, k) end\n",
    "\n",
    "evaluationIdxVal = {}\n",
    "for k = 1,#valFiles do table.insert(evaluationIdxVal, k) end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "function evaluate(net, files, indices)\n",
    "    batchSize = batchSize or 16\n",
    "    \n",
    "    if not batch or batch:size(1) ~= batchSize then\n",
    "        batch = torch.CudaTensor(batchSize, 3, dataset.h, dataset.w)\n",
    "        batchLabels = torch.CudaByteTensor(batchSize)\n",
    "\n",
    "        batchCPU = cutorch.createCudaHostFloatTensor(batch:size())\n",
    "        batchLabelsCPU = cutorch.createCudaHostByteTensor(batchLabels:size())\n",
    "\n",
    "        -- for async copy\n",
    "        cutorch.reserveStreams(1)\n",
    "        batchR, batchLabelsR = batch:clone(), batchLabels:clone()\n",
    "    end\n",
    "    \n",
    "    local threadLog = {write=function()end, close=function()end, flush=function()end}\n",
    "    -- local threadLog = io.open(outputDir .. 'threadLog.txt', 'w')\n",
    "    \n",
    "    local losses = {}\n",
    "    local avgLoss = 0\n",
    "    local confMatrix = torch.LongTensor(dataset.nClasses, dataset.nClasses):zero()\n",
    "    \n",
    "    -- Asynchronous image loading\n",
    "    threads = require 'threads'\n",
    "    threads.serialization('threads.sharedserialize')\n",
    "    threads.Threads.serialization('threads.sharedserialize')\n",
    "    require 'Queue'\n",
    "    local queue = Queue() -- a queue of preprocessed input images\n",
    "    local qMutex = threads.Mutex()\n",
    "    queue.put = threads.safe(queue.put, qMutex)\n",
    "    queue.get = threads.safe(queue.get, qMutex)\n",
    "    local loadNThreads = 6\n",
    "    \n",
    "    -- initialize load worker\n",
    "    local _data, _labels = dataset.data, dataset.labels\n",
    "    local loadPool = threads.Threads(\n",
    "        loadNThreads,\n",
    "        function(threadId)\n",
    "            (require 'sys').sleep((threadId-1) / 24)\n",
    "            dataset = require 'cifar10'\n",
    "\n",
    "            dataset.data = _data\n",
    "            dataset.labels = _labels\n",
    "            nClasses = dataset.nClasses -- 10\n",
    "            \n",
    "            torch.setnumthreads(1)\n",
    "\n",
    "            threadFiles = files\n",
    "        end,\n",
    "\n",
    "        function(threadId)\n",
    "--             print('Launching eval load thread #' .. threadId)\n",
    "        end\n",
    "    )\n",
    "    \n",
    "    local indicesIdx = 1 -- index of current test sample in `indices`\n",
    "    local numImagesProcessed = 0\n",
    "    \n",
    "    -- add some jobs\n",
    "    for _ = 1,math.min(batchSize, #indices) do\n",
    "        loadPool:addjob(\n",
    "            function(idx)\n",
    "                local img, label = dataset.loadSample(threadFiles[idx])\n",
    "                return idx, img, label\n",
    "            end,\n",
    "\n",
    "            function(idx, img, label)\n",
    "                queue:put({idx, img, label})\n",
    "            end,\n",
    "\n",
    "            indices[indicesIdx]\n",
    "        )\n",
    "        indicesIdx = indicesIdx + 1\n",
    "    end\n",
    "    \n",
    "    -- fill first batch\n",
    "    batchIndicesR = {}\n",
    "    for k = 1,math.min(batchSize, #indices) do\n",
    "        if indicesIdx <= #indices then\n",
    "            loadPool:addjob(\n",
    "                function(idx)\n",
    "                    local img, label = dataset.loadSample(threadFiles[idx])\n",
    "                    return idx, img, label\n",
    "                end,\n",
    "\n",
    "                function(idx, img, label)\n",
    "                    queue:put({idx, img, label})\n",
    "                end,\n",
    "\n",
    "                indices[indicesIdx]\n",
    "            )\n",
    "            indicesIdx = indicesIdx + 1\n",
    "        end\n",
    "\n",
    "        if queue:empty() then\n",
    "            loadPool:dojob()\n",
    "        end\n",
    "\n",
    "        if queue:empty() then -- should never enter this\n",
    "            threadLog:write('Q empty again\\n'); threadLog:flush()\n",
    "            os.execute('touch \"' .. outputDir .. 'QEMPTY\"')\n",
    "            while queue:empty() do end\n",
    "        end\n",
    "\n",
    "        local idx, input, target = table.unpack(queue:get())\n",
    "        threadLog:write('Consume\\n'); threadLog:flush()\n",
    "\n",
    "        table.insert(batchIndicesR, idx)\n",
    "        batchCPU[k]:copy(input)\n",
    "        batchLabelsCPU[k] = target\n",
    "    end\n",
    "    \n",
    "    batchR:copy(batchCPU)\n",
    "    batchLabelsR:copy(batchLabelsCPU)\n",
    "    \n",
    "    while numImagesProcessed < #indices do\n",
    "        -- prepare for filling 'standby' batch\n",
    "        cutorch.streamSynchronize(1)\n",
    "        batch, batchR = batchR, batch\n",
    "        batchLabels, batchLabelsR = batchLabelsR, batchLabels\n",
    "        batchIndices, batchIndicesR = batchIndicesR, batchIndices\n",
    "        \n",
    "        -- fill the 'standby' batch, adding image loading tasks as needed\n",
    "        batchIndicesR = {}\n",
    "        for k = 1,batchSize do\n",
    "            threadLog:write('Add,k=' .. k .. '\\n'); threadLog:flush()\n",
    "            \n",
    "            if indicesIdx <= #indices then\n",
    "                loadPool:addjob(\n",
    "                    function(idx)\n",
    "                        local img, label = dataset.loadSample(threadFiles[idx])\n",
    "                        return idx, img, label\n",
    "                    end,\n",
    "\n",
    "                    function(idx, img, label)\n",
    "                        queue:put({idx, img, label})\n",
    "                    end,\n",
    "\n",
    "                    indices[indicesIdx]\n",
    "                )\n",
    "                indicesIdx = indicesIdx + 1\n",
    "            end\n",
    "\n",
    "            if queue:empty() then\n",
    "                if loadPool:hasjob() then\n",
    "                    loadPool:dojob()\n",
    "                else\n",
    "                    -- no more files left\n",
    "                    break\n",
    "                end\n",
    "            end\n",
    "            \n",
    "            if queue:empty() then -- should never enter this\n",
    "                threadLog:write('Q empty again\\n'); threadLog:flush()\n",
    "                os.execute('touch \"' .. outputDir .. 'QEMPTY\"')\n",
    "                while queue:empty() do end\n",
    "            end\n",
    "\n",
    "            local idx, input, target = table.unpack(queue:get())\n",
    "            threadLog:write('Consume\\n'); threadLog:flush()\n",
    "\n",
    "            table.insert(batchIndicesR, idx)\n",
    "            batchCPU[k]:copy(input)\n",
    "            batchLabelsCPU[k] = target\n",
    "            k = k + 1\n",
    "        end\n",
    "\n",
    "        -- start copying the 'standby' batch to GPU\n",
    "        cutorch.setStream(1)\n",
    "        \n",
    "        batchR:copyAsync(batchCPU)\n",
    "        batchLabelsR:copyAsync(batchLabelsCPU)\n",
    "        \n",
    "        -- forward the other batch through CNN\n",
    "        cutorch.setStream(0)\n",
    "        \n",
    "        local outputs = net:forward(batch) -- batchSize x nClasses\n",
    "        \n",
    "        -- compute loss\n",
    "        batchLoss = criterion:forward(\n",
    "            outputs    [{{1, #batchIndices}}], \n",
    "            batchLabels[{{1, #batchIndices}}])\n",
    "        -- dirty fix: normalize by the actual batch size\n",
    "        batchLoss = batchLoss / #batchIndices * batchSize\n",
    "        \n",
    "        -- evaluate dataset metrics\n",
    "        local predictedLabels = select(2, outputs:max(2)):squeeze():long()\n",
    "        \n",
    "        dataset.updateConfusionMatrix(\n",
    "            confMatrix,\n",
    "            predictedLabels[{{1, #batchIndices}}],\n",
    "            batchLabels    [{{1, #batchIndices}}]:byte())\n",
    "        \n",
    "        numImagesProcessed = numImagesProcessed + #batchIndices\n",
    "        avgLoss = avgLoss + batchLoss\n",
    "    end\n",
    "    \n",
    "    local accuracy = dataset.accuracy(confMatrix)\n",
    "    avgLoss = avgLoss / math.ceil(#indices / batchSize)\n",
    "    \n",
    "    collectgarbage()\n",
    "    \n",
    "    loadPool:terminate()\n",
    "    qMutex:free()\n",
    "    threadLog:close()\n",
    "    \n",
    "    return avgLoss, accuracy, confMatrix\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "function augment(img)\n",
    "    img = torch.random() % 2 == 0 and img or image.hflip(img)\n",
    "\n",
    "    -- from https://github.com/szagoruyko/wide-residual-networks/blob/master/train.lua#L103\n",
    "    local img = imgPadder:forward(img):clone()\n",
    "    \n",
    "    local angle = (math.random() * 2 - 1) * 8.0 / 180 * math.pi\n",
    "    img = image.rotate(img, angle, 'bilinear')\n",
    "    \n",
    "    local xShift = torch.random(4, imgPadder.pad_l*2 - 2)\n",
    "    local yShift = torch.random(4, imgPadder.pad_t*2 - 2)\n",
    "    \n",
    "    return img\n",
    "        :narrow(2, yShift, dataset.h)\n",
    "        :narrow(3, xShift, dataset.w)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "function copyConvert(obj, t)\n",
    "   local copy = {}\n",
    "   for k, v in pairs(obj) do\n",
    "      if type(v) == 'table' then\n",
    "         copy[k] = copyConvert(v, t)\n",
    "      elseif torch.isTensor(v) then\n",
    "         if k == 'output' or k == 'gradInput' then\n",
    "            copy[k] = torch.Tensor():type(t)\n",
    "         else\n",
    "            copy[k] = v:type(t)\n",
    "         end\n",
    "      elseif k == '_type' then\n",
    "         copy[k] = t\n",
    "      else\n",
    "         copy[k] = v\n",
    "      end\n",
    "   end\n",
    "   if torch.typename(obj) then\n",
    "      torch.setmetatable(copy, torch.typename(obj))\n",
    "   end\n",
    "   return copy\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "-- optimStates.learningRate = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Launching thread #1\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Launching thread #2\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 0\t\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 1\t\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 2\t\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-- threadLog = io.open(outputDir .. 'threadLog.txt', 'w')\n",
    "threadLog = {write=function()end, close=function()end, flush=function()end}\n",
    "\n",
    "batchSize = 128\n",
    "shuffleEvery = math.ceil(#trainFiles / batchSize) -- shuffle data every `shuffleEvery` iterations\n",
    "outputFreq = shuffleEvery -- how often to output loss to `losses` table and to redraw loss graph\n",
    "saveFreq = shuffleEvery*2 -- how often to save `net` and `windowDebugger`s\n",
    "\n",
    "-- Asynchronous image loading\n",
    "threads = require 'threads'\n",
    "threads.serialization('threads.sharedserialize')\n",
    "threads.Threads.serialization('threads.sharedserialize')\n",
    "require 'Queue'\n",
    "local queue = Queue() -- a queue of preprocessed input images\n",
    "local qMutex = threads.Mutex()\n",
    "queue.put = threads.safe(queue.put, qMutex)\n",
    "queue.get = threads.safe(queue.get, qMutex)\n",
    "\n",
    "local nThreads = 2\n",
    "local timer = torch.Timer()\n",
    "local _data, _labels, _trainFiles, _augment = dataset.data, dataset.labels, trainFiles, augment\n",
    "\n",
    "local pool = threads.Threads(\n",
    "    nThreads,\n",
    "    function(threadId)\n",
    "        (require 'sys').sleep((threadId-1) / 7)\n",
    "        require 'nn'\n",
    "        require 'image'\n",
    "        require 'Queue'\n",
    "        threadTrainFiles = _trainFiles\n",
    "        dataset = require 'cifar10'\n",
    "        \n",
    "        torch.setnumthreads(1)\n",
    "\n",
    "        dataset.data = _data\n",
    "        dataset.labels = _labels\n",
    "        nClasses = dataset.nClasses -- 19\n",
    "        augment = _augment\n",
    "        \n",
    "        imgPadder = nn.SpatialReflectionPadding(\n",
    "            dataset.w * 0.2, dataset.w * 0.2,\n",
    "            dataset.h * 0.2, dataset.h * 0.2):float()\n",
    "    end,\n",
    "\n",
    "    function(threadId)\n",
    "        print('Launching thread #' .. threadId)\n",
    "    end\n",
    ")\n",
    "\n",
    "function randomShuffle(t)\n",
    "  for i = 1,#t do\n",
    "    local j = math.random(i, #t)\n",
    "    t[i], t[j] = t[j], t[i]\n",
    "  end\n",
    "end\n",
    "\n",
    "idx = {}\n",
    "for i = 1,#trainFiles do\n",
    "    idx[i] = i\n",
    "end\n",
    "\n",
    "local avgLoss = 0\n",
    "\n",
    "net:training()\n",
    "\n",
    "collectgarbage()\n",
    "\n",
    "if not batch or batch:size(1) ~= batchSize then\n",
    "    batch = torch.CudaTensor(batchSize, 3, dataset.h, dataset.w)\n",
    "    batchLabels = torch.CudaByteTensor(batchSize)\n",
    "    \n",
    "    batchCPU  = cutorch.createCudaHostFloatTensor(batch:size())\n",
    "    batchCPUR = cutorch.createCudaHostFloatTensor(batch:size()) -- need this for image saving\n",
    "    batchLabelsCPU  = cutorch.createCudaHostByteTensor(batchLabels:size())\n",
    "    batchLabelsCPUR = cutorch.createCudaHostByteTensor(batchLabels:size())\n",
    "    \n",
    "    -- for async copy\n",
    "    cutorch.reserveStreams(1)\n",
    "    batchR, batchLabelsR = batch:clone(), batchLabels:clone()\n",
    "end\n",
    "\n",
    "for _ = 1,24 do\n",
    "    pool:addjob(\n",
    "        function(nextFileIdx)\n",
    "            local img, label = dataset.loadSample(threadTrainFiles[nextFileIdx])\n",
    "            img = augment(img)\n",
    "            return img, label\n",
    "        end,\n",
    "\n",
    "        function(img, label)\n",
    "            queue:put({img, label})\n",
    "        end,\n",
    "\n",
    "        idx[datasetIdx]\n",
    "    )\n",
    "    datasetIdx = datasetIdx % #trainFiles + 1\n",
    "end\n",
    "\n",
    "for k = 1,batchSize do      \n",
    "    pool:dojob()\n",
    "    pool:addjob(\n",
    "        function(nextFileIdx)\n",
    "            local img, label = dataset.loadSample(threadTrainFiles[nextFileIdx])\n",
    "            img = augment(img)\n",
    "            if label == 1 and torch.random() % 2 == 0 then collectgarbage() end\n",
    "            return img, label\n",
    "        end,\n",
    "\n",
    "        function(img, label)\n",
    "            queue:put({img, label})\n",
    "        end,\n",
    "\n",
    "        idx[datasetIdx]\n",
    "    )\n",
    "    datasetIdx = datasetIdx % #trainFiles + 1\n",
    "\n",
    "    if queue:empty() then -- should never enter this\n",
    "        threadLog:write('Q empty again\\n'); threadLog:flush()\n",
    "        os.execute('touch \"' .. outputDir .. 'QEMPTY\"')\n",
    "        while queue:empty() do end\n",
    "    end\n",
    "\n",
    "    local input, target = table.unpack(queue:get())\n",
    "    threadLog:write('Consume\\n'); threadLog:flush()\n",
    "\n",
    "    batchCPU[k]:copy(input)\n",
    "    batchLabelsCPU[k] = target\n",
    "end\n",
    "\n",
    "batchR:copy(batchCPU)\n",
    "batchLabelsR:copy(batchLabelsCPU)\n",
    "\n",
    "-- ************** Main loop ***************\n",
    "\n",
    "for iter = 1,1e9 do\n",
    "    \n",
    "    if (iter-1) % shuffleEvery == 0 then\n",
    "        randomShuffle(idx)\n",
    "        print('Epoch ' .. ((iter-1) / shuffleEvery))\n",
    "    end\n",
    "    \n",
    "    if math.max(iter,1) % (40e2*outputFreq) == 0 then\n",
    "        local optimStatesList = separateParams and optimStates or {optimStates}\n",
    "        for k = 1,#optimStatesList do\n",
    "            optimStatesList[k].learningRate = optimStatesList[k].learningRate / 10\n",
    "--             optimStatesList[k].t = 0\n",
    "--             optimStatesList[k].m:zero()\n",
    "--             optimStatesList[k].v:zero()\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    if wDebs and (iter-1) % 200 == 0 then\n",
    "        for k = 1,#wDebs do\n",
    "            wDebs[k]:add(ints[k])\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    if (iter-1) % saveFreq == 0 then\n",
    "        torch.save(outputDir .. 'net.t7', \n",
    "            {copyConvert(#GPUs > 1 and net:get(1) or net, 'torch.FloatTensor'):clearState(), \n",
    "            optimStates, GSconfig})\n",
    "        \n",
    "        torch.save(outputDir .. 'evalLogger.t7', evalLogger.symbols)\n",
    "        torch.save(outputDir .. 'evalAccLogger.t7', evalAccLogger.symbols)\n",
    "        torch.save(outputDir .. 'onlineLossLogger.t7', onlineLossLogger.symbols)\n",
    "        \n",
    "        if wDebs then\n",
    "            for k = 1,#wDebs do\n",
    "                wDebs[k]:save(outputDir .. 'wd' .. k .. '.t7')\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    local batchLoss = 0 -- at current minibatch\n",
    "    \n",
    "    -- forward + backward\n",
    "    do\n",
    "        threadLog:write('other time: ' .. timer:time().real .. '\\n'); threadLog:flush(); timer:reset()\n",
    "        \n",
    "        -- start filling 'standby' batch\n",
    "        cutorch.streamSynchronize(1)\n",
    "        batch, batchR = batchR, batch\n",
    "        batchLabels, batchLabelsR = batchLabelsR, batchLabels\n",
    "        \n",
    "        threadLog:write('sync time: ' .. timer:time().real .. '\\n'); threadLog:flush(); timer:reset()\n",
    "        \n",
    "        for k = 1,batchSize do      \n",
    "            pool:addjob(\n",
    "                function(nextFileIdx)\n",
    "                    local img, label = dataset.loadSample(threadTrainFiles[nextFileIdx])\n",
    "                    img = augment(img)\n",
    "                    if label == 1 and torch.random() % 2 == 0 then collectgarbage() end\n",
    "                    return img, label\n",
    "                end,\n",
    "\n",
    "                function(img, label)\n",
    "                    queue:put({img, label})\n",
    "                end,\n",
    "\n",
    "                idx[datasetIdx]\n",
    "            )\n",
    "            datasetIdx = datasetIdx % #trainFiles + 1\n",
    "            \n",
    "            if queue:empty() then\n",
    "                threadLog:write('Q empty\\n'); threadLog:flush()\n",
    "                pool:dojob()\n",
    "            end\n",
    "\n",
    "            if queue:empty() then -- should never enter this\n",
    "                threadLog:write('Q empty again\\n'); threadLog:flush()\n",
    "                os.execute('touch \"' .. outputDir .. 'QEMPTY\"')\n",
    "                while queue:empty() do end\n",
    "            end\n",
    "\n",
    "            local input, target = table.unpack(queue:get())\n",
    "            threadLog:write('Consume\\n'); threadLog:flush()\n",
    "\n",
    "            batchCPU[k]:copy(input)\n",
    "            batchLabelsCPU[k] = target\n",
    "        end\n",
    "        threadLog:write('stby batch fill time: ' .. timer:time().real .. '\\n'); threadLog:flush(); timer:reset()\n",
    "        \n",
    "        cutorch.setStream(1)\n",
    "        \n",
    "        batchR:copyAsync(batchCPU)\n",
    "        batchLabelsR:copyAsync(batchLabelsCPU)\n",
    "        \n",
    "        cutorch.setStream(0)\n",
    "        \n",
    "--         print('start fwd')\n",
    "        threadLog:write('batch transfer time: ' .. timer:time().real .. '\\n'); threadLog:flush(); timer:reset()\n",
    "        net:forward(batch) -- batchSize x nClasses\n",
    "        batchLoss = criterion:forward(net.output, batchLabels)\n",
    "--         print('end fwd')\n",
    "        net:zeroGradParameters()\n",
    "--         print('start bwd')\n",
    "        criterion:backward(net.output, batchLabels)\n",
    "        if iter % 20 == 0 then collectgarbage() end\n",
    "        net:backward(batch, criterion.gradInput) -- accumulate gradients\n",
    "--         print('end bwd')\n",
    "        \n",
    "        threadLog:write('fwd+bwd exec time: ' .. timer:time().real .. '\\n'); threadLog:flush(); timer:reset()\n",
    "    end -- do\n",
    "    \n",
    "    -- detect NaNs\n",
    "    function hasNaN(x) return x:ne(x):sum() > 0 end\n",
    "    if batchLoss ~= batchLoss then -- or hasNaN(net.output) then\n",
    "        print('Loss is NaN')\n",
    "        break\n",
    "    end\n",
    "    \n",
    "    if iter % 10 == 0 then onlineLossLogger:add{batchLoss} end\n",
    "    if needToPlot(onlineLossLogger) then onlineLossLogger:plot() end\n",
    "--     print('start optim')\n",
    "    -- optimization step\n",
    "    if separateParams then\n",
    "        for i = 1,#params do\n",
    "            local feval = function(x)\n",
    "                return batchLoss, gradParams[i]\n",
    "            end\n",
    "            optim.adam(feval, params[i], optimStates[i])\n",
    "        end\n",
    "    else\n",
    "        local feval = function(x)\n",
    "            return batchLoss, gradParams\n",
    "        end\n",
    "        optim.adam(feval, params, optimStates)\n",
    "    end\n",
    "    threadLog:write('opt time: ' .. timer:time().real .. '\\n'); threadLog:flush(); timer:reset()\n",
    "    \n",
    "    if iter % 20 == 0 then collectgarbage() end\n",
    "    \n",
    "    if iter % outputFreq == 0 then\n",
    "        net:evaluate()\n",
    "        \n",
    "        local trainLoss, trainAcc = evaluate(net, trainFiles, evaluationIdxTrain)\n",
    "        local valLoss, valAcc = evaluate(net, valFiles, evaluationIdxVal)\n",
    "        \n",
    "        collectgarbage()\n",
    "        \n",
    "        evalLogger:add{trainLoss, valLoss}\n",
    "        evalLogger:plot()\n",
    "        evalAccLogger:add{100*(1-trainAcc), 100*(1-valAcc)}\n",
    "        evalAccLogger:plot()\n",
    "        \n",
    "        net:training()\n",
    "    end\n",
    "    \n",
    "    if lfs.attributes(outputDir .. 'INTERRUPT') then\n",
    "        os.rename(outputDir .. 'INTERRUPT', outputDir .. 'INTERRUPT_')\n",
    "        break\n",
    "    end\n",
    "end\n",
    "\n",
    "pool:terminate()\n",
    "qMutex:free()\n",
    "threadLog:close()\n",
    "\n",
    "if wDebs then\n",
    "    for k = 1,#wDebs do\n",
    "        wDebs[k]:save(outputDir .. 'wd' .. k .. '.t7')\n",
    "    end\n",
    "end\n",
    "\n",
    "collectgarbage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "local optimStatesList = separateParams and optimStates or {optimStates}\n",
    "for k = 1,#optimStatesList do\n",
    "    optimStatesList[k].learningRate = optimStatesList[k].learningRate / 2\n",
    "--     optimStatesList[k].t = 0\n",
    "--     optimStatesList[k].m:zero()\n",
    "--     optimStatesList[k].v:zero()\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1\t\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1e-3 / optimStates.learningRate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "-- do this after INTERRUPTing\n",
    "torch.save(outputDir .. 'net.t7', {copyConvert(#GPUs > 1 and net:get(1) or net, 'torch.FloatTensor'):clearState(), optimStates, GSconfig})\n",
    "\n",
    "torch.save(outputDir .. 'evalLogger.t7', evalLogger.symbols)\n",
    "torch.save(outputDir .. 'evalAccLogger.t7', evalAccLogger.symbols)\n",
    "torch.save(outputDir .. 'onlineLossLogger.t7', onlineLossLogger.symbols)\n",
    "\n",
    "if wDebs then\n",
    "    for k = 1,#wDebs do\n",
    "        wDebs[k]:exportVideo(outputDir .. 'int-layer-' .. k .. '.avi')\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export images for evaluation by `cityscapesScripts`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "false\t\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "nil\tnil\t\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "46.555427074432 seconds\t\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local timer = torch.Timer()\n",
    "net:evaluate()\n",
    "print(net.train)\n",
    "\n",
    "valLoss, valAcc, confMatrix = evaluate(net, valFiles, evaluationIdxVal)\n",
    "\n",
    "net:training()\n",
    "print(timer:time().real .. ' seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98914853565462\t\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.exp(-valLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iTorch",
   "language": "lua",
   "name": "itorch"
  },
  "language_info": {
   "name": "lua",
   "version": "5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
